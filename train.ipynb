{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fe1ec7",
   "metadata": {},
   "source": [
    "# Kaggle Dogs VS Cats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce542f",
   "metadata": {},
   "source": [
    "## Basic Lenet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0359724e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 23:35:02.535419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 23:35:02.546979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-05 23:35:02.547517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    \n",
    "from tensorflow.keras import layers, callbacks, metrics, optimizers, models\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import data_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26f4f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogCatSequence(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, files_folder, files_list, batch_size):\n",
    "        if batch_size > len(files_list):\n",
    "            raise ValueError('Batch size is bigger than length of files list')\n",
    "            \n",
    "        self._files_list = files_list\n",
    "        self._batch_size = batch_size\n",
    "        self._files_folder = files_folder\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        end = min((index + 1) * self._batch_size, len(self._files_list) - 1)\n",
    "        batch_files_list = self._files_list[index * self._batch_size: end]\n",
    "\n",
    "        batch_x = np.array([io.imread(os.path.join(self._files_folder, filename)) for filename in batch_files_list])\n",
    "        batch_y = np.array([float(filename.startswith('dog')) for filename in batch_files_list])\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self._files_list) / self._batch_size))\n",
    "\n",
    "\n",
    "def f_score(ytrue, ypred, threshold=0.5, epsilon=10e-7):\n",
    "    # casting ytrue and ypred as float dtype\n",
    "    ytrue = tf.cast(ytrue, tf.float32)\n",
    "    ypred = tf.cast(ypred, tf.float32)\n",
    "\n",
    "    # setting values of ypred greater than the set threshold to 1 while those lesser to 0\n",
    "    ypred = tf.cast(tf.greater_equal(ypred, tf.constant(threshold)), tf.float32)\n",
    "\n",
    "    tp = tf.reduce_sum(ytrue*ypred) # calculating true positives\n",
    "    predicted_positive = tf.reduce_sum(ypred) # calculating predicted positives\n",
    "    actual_positive = tf.reduce_sum(ytrue) # calculating actual positives\n",
    "    \n",
    "    precision = tp/(predicted_positive+epsilon) # calculating precision\n",
    "    recall = tp/(actual_positive+epsilon) # calculating recall\n",
    "    \n",
    "    # calculating fbeta\n",
    "    fb = 2 * precision*recall / (precision + recall + epsilon)\n",
    "\n",
    "    return fb\n",
    "\n",
    "\n",
    "def print_errors(model, train_seq, test_seq):\n",
    "    train_f_score = model.evaluate(train_seq)[1]\n",
    "    test_f_score = model.evaluate(test_seq)[1]\n",
    "\n",
    "    print('train error = ', 1 - train_f_score)\n",
    "    print('test error = ', 1 - test_f_score)\n",
    "    \n",
    "    \n",
    "def train_model(model):\n",
    "    model_check_point = callbacks.ModelCheckpoint(filepath='dogs_vs_cats_lenet5_weights_{epoch}.h5',\n",
    "                                              save_weights_only=True,\n",
    "                                              save_best_only=True,\n",
    "                                              monitor='val_loss',\n",
    "                                              mode='min',\n",
    "                                              verbose=True)\n",
    "\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                            mode='min',\n",
    "                                            min_delta=10e-3,\n",
    "                                            verbose=True,\n",
    "                                            patience=10,\n",
    "                                            restore_best_weights=True)\n",
    "\n",
    "\n",
    "    train_epochs = 100\n",
    "\n",
    "    train_callbacks = [model_check_point, early_stopping]\n",
    "\n",
    "    model.fit(train_seq, epochs=train_epochs, validation_data=val_seq, callbacks=train_callbacks)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def tune_model(model_func, train_sequence, validation_sequence):\n",
    "    hp = kt.HyperParameters()\n",
    "\n",
    "    tuner = kt.Hyperband(\n",
    "        model_func,\n",
    "        objective=\"val_loss\",\n",
    "        max_epochs=15,\n",
    "        directory='tuner',\n",
    "        project_name=str(model_func),\n",
    "    )\n",
    "\n",
    "    train_epochs = 10\n",
    "    tuner.search(train_sequence, epochs=train_epochs, validation_data=validation_sequence)\n",
    "    \n",
    "    return tuner\n",
    "\n",
    "def get_dataset_sequences(images_folder, batch_size):\n",
    "    images_list = sorted(os.listdir(images_folder))\n",
    "    random.seed(42)\n",
    "    random.shuffle(images_list)\n",
    "\n",
    "    train_part = 0.9\n",
    "    val_part = 0.05\n",
    "    test_part = 0.05\n",
    "\n",
    "    get_last_index = lambda part: int(part * len(images_list))\n",
    "\n",
    "    train_images_list = images_list[: get_last_index(train_part)]\n",
    "    val_images_list = images_list[get_last_index(train_part): get_last_index(train_part + val_part)]\n",
    "    test_images_list = images_list[get_last_index(train_part + val_part):\n",
    "                                       get_last_index(train_part + val_part + test_part)]\n",
    "    train_seq = DogCatSequence(images_folder, train_images_list, batch_size)\n",
    "    val_seq = DogCatSequence(images_folder, val_images_list, batch_size)\n",
    "    test_seq = DogCatSequence(images_folder, test_images_list, batch_size)\n",
    "    \n",
    "    return train_seq, val_seq, test_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2893ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq, val_seq, test_seq = get_dataset_sequences('train-64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8973ddf5",
   "metadata": {},
   "source": [
    "## Basic Lenet-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a1f607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet5_v1(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    nn = layers.Rescaling(1.0 / 255.0)(inputs)\n",
    "    nn = layers.Conv2D(filters=6, kernel_size=5, strides=(1, 1), activation='relu')(inputs)\n",
    "    nn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(nn)\n",
    "    nn = layers.Conv2D(filters=16, kernel_size=5, strides=(1, 1), activation='relu')(nn)\n",
    "    nn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(nn)\n",
    "    nn = layers.Flatten()(nn)\n",
    "    nn = layers.Dense(400, activation='relu')(nn)\n",
    "    nn = layers.Dense(120, activation='relu')(nn)\n",
    "    nn = layers.Dense(84, activation='relu')(nn)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(nn)\n",
    "    return models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8355bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lenet5_v1(train_seq[0][0].shape[1:])\n",
    "\n",
    "train_metrics = [f_score,\n",
    "                 metrics.Precision(thresholds=0.5),\n",
    "                 metrics.Recall(thresholds=0.5)]\n",
    "\n",
    "train_optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "train_loss = 'binary_crossentropy'\n",
    "\n",
    "model.compile(loss=train_loss,\n",
    "              optimizer=train_optimizer,\n",
    "              metrics=train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32331196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 1.4011 - f_score: 0.5428 - precision_1: 0.5430 - recall_1: 0.6137\n",
      "Epoch 1: val_loss improved from inf to 0.68042, saving model to dogs_vs_cats_lenet5_weights_1.h5\n",
      "352/352 [==============================] - 11s 29ms/step - loss: 1.3990 - f_score: 0.5430 - precision_1: 0.5430 - recall_1: 0.6139 - val_loss: 0.6804 - val_f_score: 0.5652 - val_precision_1: 0.5822 - val_recall_1: 0.5584\n",
      "Epoch 2/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.6473 - f_score: 0.6199 - precision_1: 0.6176 - recall_1: 0.6442\n",
      "Epoch 2: val_loss improved from 0.68042 to 0.65341, saving model to dogs_vs_cats_lenet5_weights_2.h5\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.6474 - f_score: 0.6202 - precision_1: 0.6180 - recall_1: 0.6447 - val_loss: 0.6534 - val_f_score: 0.6624 - val_precision_1: 0.5955 - val_recall_1: 0.7476\n",
      "Epoch 3/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.6033 - f_score: 0.6684 - precision_1: 0.6626 - recall_1: 0.6893\n",
      "Epoch 3: val_loss improved from 0.65341 to 0.64378, saving model to dogs_vs_cats_lenet5_weights_3.h5\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.6033 - f_score: 0.6684 - precision_1: 0.6626 - recall_1: 0.6893 - val_loss: 0.6438 - val_f_score: 0.6702 - val_precision_1: 0.6291 - val_recall_1: 0.7224\n",
      "Epoch 4/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.5366 - f_score: 0.7250 - precision_1: 0.7186 - recall_1: 0.7387\n",
      "Epoch 4: val_loss did not improve from 0.64378\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.5366 - f_score: 0.7250 - precision_1: 0.7186 - recall_1: 0.7387 - val_loss: 0.6653 - val_f_score: 0.6368 - val_precision_1: 0.6519 - val_recall_1: 0.6262\n",
      "Epoch 5/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.4584 - f_score: 0.7762 - precision_1: 0.7752 - recall_1: 0.7839\n",
      "Epoch 5: val_loss did not improve from 0.64378\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.4583 - f_score: 0.7761 - precision_1: 0.7752 - recall_1: 0.7839 - val_loss: 0.7434 - val_f_score: 0.6477 - val_precision_1: 0.6420 - val_recall_1: 0.6562\n",
      "Epoch 6/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.3632 - f_score: 0.8310 - precision_1: 0.8365 - recall_1: 0.8307\n",
      "Epoch 6: val_loss did not improve from 0.64378\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.3632 - f_score: 0.8310 - precision_1: 0.8365 - recall_1: 0.8307 - val_loss: 0.8072 - val_f_score: 0.6457 - val_precision_1: 0.6343 - val_recall_1: 0.6593\n",
      "Epoch 7/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.2775 - f_score: 0.8747 - precision_1: 0.8824 - recall_1: 0.8700\n",
      "Epoch 7: val_loss did not improve from 0.64378\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.2775 - f_score: 0.8747 - precision_1: 0.8824 - recall_1: 0.8700 - val_loss: 0.9918 - val_f_score: 0.6228 - val_precision_1: 0.6254 - val_recall_1: 0.6215\n",
      "Epoch 8/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.2416 - f_score: 0.8948 - precision_1: 0.9024 - recall_1: 0.8910\n",
      "Epoch 8: val_loss did not improve from 0.64378\n",
      "352/352 [==============================] - 10s 29ms/step - loss: 0.2416 - f_score: 0.8948 - precision_1: 0.9024 - recall_1: 0.8910 - val_loss: 1.0185 - val_f_score: 0.6347 - val_precision_1: 0.6655 - val_recall_1: 0.6057\n",
      "Epoch 9/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.2049 - f_score: 0.9157 - precision_1: 0.9229 - recall_1: 0.9115\n",
      "Epoch 9: val_loss did not improve from 0.64378\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.2049 - f_score: 0.9157 - precision_1: 0.9229 - recall_1: 0.9115 - val_loss: 1.2297 - val_f_score: 0.6216 - val_precision_1: 0.6371 - val_recall_1: 0.6120\n",
      "Epoch 10/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.1534 - f_score: 0.9382 - precision_1: 0.9434 - recall_1: 0.9356\n",
      "Epoch 10: val_loss did not improve from 0.64378\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.1534 - f_score: 0.9382 - precision_1: 0.9434 - recall_1: 0.9356 - val_loss: 1.5451 - val_f_score: 0.6012 - val_precision_1: 0.6491 - val_recall_1: 0.5631\n",
      "Epoch 11/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.1407 - f_score: 0.9478 - precision_1: 0.9538 - recall_1: 0.9436\n",
      "Epoch 11: val_loss did not improve from 0.64378\n",
      "352/352 [==============================] - 10s 29ms/step - loss: 0.1407 - f_score: 0.9478 - precision_1: 0.9538 - recall_1: 0.9436 - val_loss: 1.4477 - val_f_score: 0.6411 - val_precision_1: 0.6650 - val_recall_1: 0.6199\n",
      "Epoch 12/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.1379 - f_score: 0.9502 - precision_1: 0.9557 - recall_1: 0.9465\n",
      "Epoch 12: val_loss did not improve from 0.64378\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.1382 - f_score: 0.9499 - precision_1: 0.9558 - recall_1: 0.9460 - val_loss: 1.5828 - val_f_score: 0.6341 - val_precision_1: 0.6684 - val_recall_1: 0.6041\n",
      "Epoch 12: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7fa3a9944d00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2da1c13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 10s 27ms/step - loss: 0.5957 - f_score: 0.7138 - precision_1: 0.6439 - recall_1: 0.8072\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 0.6495 - f_score: 0.6705 - precision_1: 0.6068 - recall_1: 0.7595\n",
      "train error =  0.2862144112586975\n",
      "test error =  0.329542338848114\n"
     ]
    }
   ],
   "source": [
    "print_errors(model, train_seq, test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc47dfab",
   "metadata": {},
   "source": [
    "## Basic Lenet-5 with tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dc727ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet5_v2(hp):\n",
    "    \n",
    "    inputs = layers.Input(shape=(64, 64, 3))\n",
    "    nn = layers.Rescaling(1.0 / 255.0)(inputs)\n",
    "\n",
    "    nn = layers.Conv2D(filters=hp.Int(name='filters_conv_1', min_value=1, max_value=10, step=1),\n",
    "                       kernel_size=5, strides=(1, 1), activation='relu')(inputs)\n",
    "    nn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(nn)\n",
    "\n",
    "    \n",
    "    nn = layers.Conv2D(filters=hp.Int(name='filters_conv_2', min_value=10, max_value=20, step=1),\n",
    "                       kernel_size=5, strides=(1, 1), activation='relu')(nn)\n",
    "    nn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(nn)\n",
    "    \n",
    "    nn = layers.Flatten()(nn)\n",
    "\n",
    "    nn = layers.Dense(hp.Int(name='units_dense_1', min_value=300, max_value=500, step=1), activation='relu')(nn)\n",
    "    nn = layers.Dense(hp.Int(name='units_dense_2', min_value=50, max_value=120, step=1), activation='relu')(nn)\n",
    "    nn = layers.Dense(hp.Int(name='units_dense_3', min_value=60, max_value=100, step=1), activation='relu')(nn)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(nn)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    train_metrics = [f_score,\n",
    "                     metrics.Precision(thresholds=0.5),\n",
    "                     metrics.Recall(thresholds=0.5)]\n",
    "\n",
    "    train_optimizer = optimizers.Adam(\n",
    "        learning_rate=hp.Float(name='learning_rate', min_value=10e-6, max_value=10e-4, sampling='log'))\n",
    "    train_loss = 'binary_crossentropy'\n",
    "\n",
    "    model.compile(loss=train_loss, optimizer=train_optimizer, metrics=train_metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcb24963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26ce2d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 02m 32s]\n",
      "val_loss: 0.5534921884536743\n",
      "\n",
      "Best val_loss So Far: 0.515401303768158\n",
      "Total elapsed time: 00h 22m 10s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner = tune_model(lenet5_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40bc3870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.4684 - f_score: 0.7750 - precision: 0.7768 - recall: 0.7819\n",
      "Epoch 1: val_loss improved from inf to 0.50380, saving model to dogs_vs_cats_lenet5_weights_1.h5\n",
      "352/352 [==============================] - 11s 29ms/step - loss: 0.4685 - f_score: 0.7751 - precision: 0.7766 - recall: 0.7822 - val_loss: 0.5038 - val_f_score: 0.7344 - val_precision: 0.8192 - val_recall: 0.6719\n",
      "Epoch 2/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.4109 - f_score: 0.8078 - precision: 0.8117 - recall: 0.8113\n",
      "Epoch 2: val_loss did not improve from 0.50380\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.4109 - f_score: 0.8078 - precision: 0.8117 - recall: 0.8113 - val_loss: 0.5213 - val_f_score: 0.7787 - val_precision: 0.7263 - val_recall: 0.8454\n",
      "Epoch 3/100\n",
      "350/352 [============================>.] - ETA: 0s - loss: 0.3421 - f_score: 0.8461 - precision: 0.8458 - recall: 0.8524\n",
      "Epoch 3: val_loss did not improve from 0.50380\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.3414 - f_score: 0.8467 - precision: 0.8463 - recall: 0.8532 - val_loss: 0.5323 - val_f_score: 0.7662 - val_precision: 0.7402 - val_recall: 0.8044\n",
      "Epoch 4/100\n",
      "350/352 [============================>.] - ETA: 0s - loss: 0.2354 - f_score: 0.9024 - precision: 0.9011 - recall: 0.9063\n",
      "Epoch 4: val_loss did not improve from 0.50380\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.2355 - f_score: 0.9023 - precision: 0.9007 - recall: 0.9063 - val_loss: 0.5708 - val_f_score: 0.7588 - val_precision: 0.7473 - val_recall: 0.7744\n",
      "Epoch 5/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.1659 - f_score: 0.9347 - precision: 0.9322 - recall: 0.9397\n",
      "Epoch 5: val_loss did not improve from 0.50380\n",
      "352/352 [==============================] - 10s 29ms/step - loss: 0.1666 - f_score: 0.9343 - precision: 0.9320 - recall: 0.9390 - val_loss: 0.6923 - val_f_score: 0.7287 - val_precision: 0.7754 - val_recall: 0.6972\n",
      "Epoch 6/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.1210 - f_score: 0.9537 - precision: 0.9520 - recall: 0.9560\n",
      "Epoch 6: val_loss did not improve from 0.50380\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.1210 - f_score: 0.9537 - precision: 0.9520 - recall: 0.9560 - val_loss: 0.8316 - val_f_score: 0.7400 - val_precision: 0.7452 - val_recall: 0.7382\n",
      "Epoch 7/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.0917 - f_score: 0.9664 - precision: 0.9666 - recall: 0.9672\n",
      "Epoch 7: val_loss did not improve from 0.50380\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.0917 - f_score: 0.9664 - precision: 0.9666 - recall: 0.9672 - val_loss: 0.9645 - val_f_score: 0.7504 - val_precision: 0.7415 - val_recall: 0.7603\n",
      "Epoch 8/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.0792 - f_score: 0.9718 - precision: 0.9712 - recall: 0.9734\n",
      "Epoch 8: val_loss did not improve from 0.50380\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.0792 - f_score: 0.9718 - precision: 0.9712 - recall: 0.9734 - val_loss: 1.0124 - val_f_score: 0.7519 - val_precision: 0.7831 - val_recall: 0.7287\n",
      "Epoch 9/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.0814 - f_score: 0.9697 - precision: 0.9692 - recall: 0.9708\n",
      "Epoch 9: val_loss did not improve from 0.50380\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.0814 - f_score: 0.9697 - precision: 0.9692 - recall: 0.9708 - val_loss: 1.0356 - val_f_score: 0.7287 - val_precision: 0.7641 - val_recall: 0.7050\n",
      "Epoch 10/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.0560 - f_score: 0.9805 - precision: 0.9807 - recall: 0.9809\n",
      "Epoch 10: val_loss did not improve from 0.50380\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.0563 - f_score: 0.9803 - precision: 0.9807 - recall: 0.9806 - val_loss: 1.0238 - val_f_score: 0.7549 - val_precision: 0.7256 - val_recall: 0.7965\n",
      "Epoch 11/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.0668 - f_score: 0.9760 - precision: 0.9758 - recall: 0.9763\n",
      "Epoch 11: val_loss did not improve from 0.50380\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "352/352 [==============================] - 10s 28ms/step - loss: 0.0667 - f_score: 0.9760 - precision: 0.9759 - recall: 0.9762 - val_loss: 1.0474 - val_f_score: 0.7641 - val_precision: 0.7897 - val_recall: 0.7524\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "lenet5_v2_best_model = tuner.get_best_models()[0]\n",
    "lenet5_v2_best_model = train_model(lenet5_v2_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb6fdafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 10s 27ms/step - loss: 0.4086 - f_score: 0.8071 - precision: 0.8800 - recall: 0.7505\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 0.5472 - f_score: 0.6901 - precision: 0.7789 - recall: 0.6297\n",
      "train error =  0.1928519606590271\n",
      "test error =  0.3098623752593994\n"
     ]
    }
   ],
   "source": [
    "print_errors(lenet5_v2_best_model, train_seq, test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d032c6a",
   "metadata": {},
   "source": [
    "## Lenet-5 with additional conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aee38b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet5_v3(hp):\n",
    "    \n",
    "    inputs = layers.Input(shape=(64, 64, 3))\n",
    "    nn = layers.Rescaling(1.0 / 255.0)(inputs)\n",
    "\n",
    "    nn = layers.Conv2D(filters=hp.Int(name='filters_conv_1', min_value=1, max_value=20, step=1),\n",
    "                       kernel_size=5, strides=(1, 1), activation='relu')(inputs)\n",
    "    nn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(nn)\n",
    "\n",
    "    \n",
    "    nn = layers.Conv2D(filters=hp.Int(name='filters_conv_2', min_value=10, max_value=30, step=1),\n",
    "                       kernel_size=5, strides=(1, 1), activation='relu')(nn)\n",
    "    nn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(nn)\n",
    "\n",
    "    \n",
    "    nn = layers.Conv2D(filters=hp.Int(name='filters_conv_3', min_value=20, max_value=40, step=1),\n",
    "                       kernel_size=5, strides=(1, 1), activation='relu')(nn)\n",
    "    nn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(nn)\n",
    "\n",
    "    \n",
    "    nn = layers.Flatten()(nn)\n",
    "\n",
    "    nn = layers.Dense(hp.Int(name='units_dense_1', min_value=300, max_value=500, step=1), activation='relu')(nn)\n",
    "    nn = layers.Dense(hp.Int(name='units_dense_2', min_value=50, max_value=120, step=1), activation='relu')(nn)\n",
    "    nn = layers.Dense(hp.Int(name='units_dense_3', min_value=60, max_value=100, step=1), activation='relu')(nn)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(nn)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    train_metrics = [f_score,\n",
    "                     metrics.Precision(thresholds=0.5),\n",
    "                     metrics.Recall(thresholds=0.5)]\n",
    "\n",
    "    train_optimizer = optimizers.Adam(\n",
    "        learning_rate=hp.Float(name='learning_rate', min_value=10e-6, max_value=10e-4, sampling='log'))\n",
    "    train_loss = 'binary_crossentropy'\n",
    "\n",
    "    model.compile(loss=train_loss, optimizer=train_optimizer, metrics=train_metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48e8a2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 02m 34s]\n",
      "val_loss: 0.5273486375808716\n",
      "\n",
      "Best val_loss So Far: 0.4456341564655304\n",
      "Total elapsed time: 00h 29m 04s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner = tune_model(lenet5_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b00a02cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.3684 - f_score: 0.8283 - precision: 0.8333 - recall: 0.8299\n",
      "Epoch 1: val_loss improved from inf to 0.43387, saving model to dogs_vs_cats_lenet5_weights_1.h5\n",
      "352/352 [==============================] - 11s 30ms/step - loss: 0.3684 - f_score: 0.8284 - precision: 0.8335 - recall: 0.8301 - val_loss: 0.4339 - val_f_score: 0.8081 - val_precision: 0.7725 - val_recall: 0.8517\n",
      "Epoch 2/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.3341 - f_score: 0.8490 - precision: 0.8499 - recall: 0.8529\n",
      "Epoch 2: val_loss did not improve from 0.43387\n",
      "352/352 [==============================] - 10s 29ms/step - loss: 0.3345 - f_score: 0.8488 - precision: 0.8502 - recall: 0.8523 - val_loss: 0.4832 - val_f_score: 0.7683 - val_precision: 0.8349 - val_recall: 0.7177\n",
      "Epoch 3/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.3072 - f_score: 0.8629 - precision: 0.8626 - recall: 0.8686\n",
      "Epoch 3: val_loss did not improve from 0.43387\n",
      "352/352 [==============================] - 10s 29ms/step - loss: 0.3070 - f_score: 0.8630 - precision: 0.8626 - recall: 0.8689 - val_loss: 0.4764 - val_f_score: 0.7952 - val_precision: 0.8255 - val_recall: 0.7760\n",
      "Epoch 4/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.2640 - f_score: 0.8819 - precision: 0.8848 - recall: 0.8830\n",
      "Epoch 4: val_loss did not improve from 0.43387\n",
      "352/352 [==============================] - 10s 29ms/step - loss: 0.2640 - f_score: 0.8819 - precision: 0.8848 - recall: 0.8830 - val_loss: 0.5443 - val_f_score: 0.7747 - val_precision: 0.8168 - val_recall: 0.7382\n",
      "Epoch 5/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.2337 - f_score: 0.8992 - precision: 0.9002 - recall: 0.9019\n",
      "Epoch 5: val_loss did not improve from 0.43387\n",
      "352/352 [==============================] - 10s 29ms/step - loss: 0.2335 - f_score: 0.8992 - precision: 0.9002 - recall: 0.9019 - val_loss: 0.5956 - val_f_score: 0.7604 - val_precision: 0.8702 - val_recall: 0.6767\n",
      "Epoch 6/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.2135 - f_score: 0.9091 - precision: 0.9071 - recall: 0.9139\n",
      "Epoch 6: val_loss did not improve from 0.43387\n",
      "352/352 [==============================] - 10s 29ms/step - loss: 0.2134 - f_score: 0.9091 - precision: 0.9071 - recall: 0.9139 - val_loss: 0.5727 - val_f_score: 0.7830 - val_precision: 0.7917 - val_recall: 0.7792\n",
      "Epoch 7/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.1727 - f_score: 0.9274 - precision: 0.9289 - recall: 0.9282\n",
      "Epoch 7: val_loss did not improve from 0.43387\n",
      "352/352 [==============================] - 10s 29ms/step - loss: 0.1727 - f_score: 0.9273 - precision: 0.9289 - recall: 0.9281 - val_loss: 0.6221 - val_f_score: 0.7859 - val_precision: 0.7889 - val_recall: 0.7839\n",
      "Epoch 8/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.1679 - f_score: 0.9290 - precision: 0.9311 - recall: 0.9304\n",
      "Epoch 8: val_loss did not improve from 0.43387\n",
      "352/352 [==============================] - 10s 29ms/step - loss: 0.1679 - f_score: 0.9290 - precision: 0.9311 - recall: 0.9304 - val_loss: 0.6539 - val_f_score: 0.7819 - val_precision: 0.8063 - val_recall: 0.7618\n",
      "Epoch 9/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.1366 - f_score: 0.9450 - precision: 0.9448 - recall: 0.9474\n",
      "Epoch 9: val_loss did not improve from 0.43387\n",
      "352/352 [==============================] - 10s 29ms/step - loss: 0.1369 - f_score: 0.9447 - precision: 0.9448 - recall: 0.9469 - val_loss: 0.8342 - val_f_score: 0.7273 - val_precision: 0.8520 - val_recall: 0.6356\n",
      "Epoch 10/100\n",
      "351/352 [============================>.] - ETA: 0s - loss: 0.1232 - f_score: 0.9516 - precision: 0.9521 - recall: 0.9532\n",
      "Epoch 10: val_loss did not improve from 0.43387\n",
      "352/352 [==============================] - 10s 29ms/step - loss: 0.1234 - f_score: 0.9516 - precision: 0.9521 - recall: 0.9532 - val_loss: 0.7854 - val_f_score: 0.7417 - val_precision: 0.8229 - val_recall: 0.6814\n",
      "Epoch 11/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.1152 - f_score: 0.9544 - precision: 0.9552 - recall: 0.9550\n",
      "Epoch 11: val_loss did not improve from 0.43387\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "352/352 [==============================] - 10s 29ms/step - loss: 0.1152 - f_score: 0.9544 - precision: 0.9552 - recall: 0.9550 - val_loss: 0.9225 - val_f_score: 0.7254 - val_precision: 0.8441 - val_recall: 0.6404\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "lenet5_v3_best_model = tuner.get_best_models()[0]\n",
    "lenet5_v3_best_model = train_model(lenet5_v3_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08bb40b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 10s 27ms/step - loss: 0.3331 - f_score: 0.8587 - precision: 0.8155 - recall: 0.9109\n",
      "20/20 [==============================] - 1s 27ms/step - loss: 0.4642 - f_score: 0.7864 - precision: 0.7400 - recall: 0.8465\n",
      "train error =  0.14125597476959229\n",
      "test error =  0.21361839771270752\n"
     ]
    }
   ],
   "source": [
    "print_errors(lenet5_v3_best_model, train_seq, test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c2fc7c",
   "metadata": {},
   "source": [
    "## Lenet5 with two additional conv layers and 128x128 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b437d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "train_seq, val_seq, test_seq = get_dataset_sequences('train-128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1595437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet5_v4(hp):\n",
    "    \n",
    "    inputs = layers.Input(shape=(128, 128, 3))\n",
    "    nn = layers.Rescaling(1.0 / 255.0)(inputs)\n",
    "\n",
    "    nn = layers.Conv2D(filters=hp.Int(name='filters_conv_1', min_value=1, max_value=20, step=1),\n",
    "                       kernel_size=5, strides=(1, 1), activation='relu')(inputs)\n",
    "    nn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(nn)\n",
    "\n",
    "    \n",
    "    nn = layers.Conv2D(filters=hp.Int(name='filters_conv_2', min_value=20, max_value=40, step=1),\n",
    "                       kernel_size=5, strides=(1, 1), activation='relu')(nn)\n",
    "    nn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(nn)\n",
    "\n",
    "    \n",
    "    nn = layers.Conv2D(filters=hp.Int(name='filters_conv_3', min_value=40, max_value=60, step=1),\n",
    "                       kernel_size=5, strides=(1, 1), activation='relu')(nn)\n",
    "    nn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(nn)\n",
    "    \n",
    "    nn = layers.Conv2D(filters=hp.Int(name='filters_conv_4', min_value=60, max_value=80, step=1),\n",
    "                       kernel_size=5, strides=(1, 1), activation='relu')(nn)\n",
    "    nn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(nn)\n",
    "\n",
    "    \n",
    "    nn = layers.Flatten()(nn)\n",
    "\n",
    "    nn = layers.Dense(hp.Int(name='units_dense_1', min_value=300, max_value=500, step=1), activation='relu')(nn)\n",
    "    nn = layers.Dense(hp.Int(name='units_dense_2', min_value=80, max_value=120, step=1), activation='relu')(nn)\n",
    "    nn = layers.Dense(hp.Int(name='units_dense_3', min_value=40, max_value=80, step=1), activation='relu')(nn)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(nn)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    train_metrics = [f_score,\n",
    "                     metrics.Precision(thresholds=0.5),\n",
    "                     metrics.Recall(thresholds=0.5)]\n",
    "\n",
    "    train_optimizer = optimizers.Adam(\n",
    "        learning_rate=hp.Float(name='learning_rate', min_value=10e-6, max_value=10e-4, sampling='log'))\n",
    "    train_loss = 'binary_crossentropy'\n",
    "\n",
    "    model.compile(loss=train_loss, optimizer=train_optimizer, metrics=train_metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4114194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 07m 11s]\n",
      "val_loss: 0.38811683654785156\n",
      "\n",
      "Best val_loss So Far: 0.35724955797195435\n",
      "Total elapsed time: 01h 09m 22s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner = tune_model(lenet5_v4, train_seq, val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70e1d3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.2491 - f_score: 0.8883 - precision: 0.8867 - recall: 0.8942\n",
      "Epoch 1: val_loss improved from inf to 0.33639, saving model to dogs_vs_cats_lenet5_weights_1.h5\n",
      "352/352 [==============================] - 28s 78ms/step - loss: 0.2491 - f_score: 0.8883 - precision: 0.8867 - recall: 0.8942 - val_loss: 0.3364 - val_f_score: 0.8575 - val_precision: 0.8460 - val_recall: 0.8754\n",
      "Epoch 2/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.2141 - f_score: 0.9077 - precision: 0.9054 - recall: 0.9130\n",
      "Epoch 2: val_loss did not improve from 0.33639\n",
      "352/352 [==============================] - 27s 77ms/step - loss: 0.2141 - f_score: 0.9077 - precision: 0.9054 - recall: 0.9130 - val_loss: 0.4146 - val_f_score: 0.8201 - val_precision: 0.9074 - val_recall: 0.7571\n",
      "Epoch 3/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.1729 - f_score: 0.9270 - precision: 0.9266 - recall: 0.9304\n",
      "Epoch 3: val_loss did not improve from 0.33639\n",
      "352/352 [==============================] - 27s 77ms/step - loss: 0.1729 - f_score: 0.9270 - precision: 0.9266 - recall: 0.9304 - val_loss: 0.3794 - val_f_score: 0.8401 - val_precision: 0.8189 - val_recall: 0.8628\n",
      "Epoch 4/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.1561 - f_score: 0.9373 - precision: 0.9344 - recall: 0.9421\n",
      "Epoch 4: val_loss did not improve from 0.33639\n",
      "352/352 [==============================] - 27s 77ms/step - loss: 0.1561 - f_score: 0.9373 - precision: 0.9344 - recall: 0.9421 - val_loss: 0.4124 - val_f_score: 0.8451 - val_precision: 0.8476 - val_recall: 0.8423\n",
      "Epoch 5/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.1333 - f_score: 0.9467 - precision: 0.9444 - recall: 0.9499\n",
      "Epoch 5: val_loss did not improve from 0.33639\n",
      "352/352 [==============================] - 27s 77ms/step - loss: 0.1333 - f_score: 0.9467 - precision: 0.9444 - recall: 0.9499 - val_loss: 0.4849 - val_f_score: 0.8418 - val_precision: 0.8548 - val_recall: 0.8360\n",
      "Epoch 6/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.1153 - f_score: 0.9543 - precision: 0.9521 - recall: 0.9579\n",
      "Epoch 6: val_loss did not improve from 0.33639\n",
      "352/352 [==============================] - 27s 77ms/step - loss: 0.1153 - f_score: 0.9543 - precision: 0.9521 - recall: 0.9579 - val_loss: 0.4740 - val_f_score: 0.8514 - val_precision: 0.8192 - val_recall: 0.8864\n",
      "Epoch 7/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.1012 - f_score: 0.9605 - precision: 0.9581 - recall: 0.9644\n",
      "Epoch 7: val_loss did not improve from 0.33639\n",
      "352/352 [==============================] - 27s 77ms/step - loss: 0.1012 - f_score: 0.9605 - precision: 0.9581 - recall: 0.9644 - val_loss: 0.4687 - val_f_score: 0.8489 - val_precision: 0.8893 - val_recall: 0.8107\n",
      "Epoch 8/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.0836 - f_score: 0.9675 - precision: 0.9662 - recall: 0.9697\n",
      "Epoch 8: val_loss did not improve from 0.33639\n",
      "352/352 [==============================] - 27s 77ms/step - loss: 0.0836 - f_score: 0.9675 - precision: 0.9662 - recall: 0.9697 - val_loss: 0.4809 - val_f_score: 0.8623 - val_precision: 0.8380 - val_recall: 0.8896\n",
      "Epoch 9/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.0817 - f_score: 0.9685 - precision: 0.9679 - recall: 0.9703\n",
      "Epoch 9: val_loss did not improve from 0.33639\n",
      "352/352 [==============================] - 27s 77ms/step - loss: 0.0817 - f_score: 0.9685 - precision: 0.9679 - recall: 0.9703 - val_loss: 0.5997 - val_f_score: 0.8382 - val_precision: 0.9053 - val_recall: 0.7839\n",
      "Epoch 10/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.0780 - f_score: 0.9692 - precision: 0.9688 - recall: 0.9703\n",
      "Epoch 10: val_loss did not improve from 0.33639\n",
      "352/352 [==============================] - 27s 77ms/step - loss: 0.0780 - f_score: 0.9692 - precision: 0.9688 - recall: 0.9703 - val_loss: 0.5070 - val_f_score: 0.8481 - val_precision: 0.8549 - val_recall: 0.8454\n",
      "Epoch 11/100\n",
      "352/352 [==============================] - ETA: 0s - loss: 0.0710 - f_score: 0.9732 - precision: 0.9730 - recall: 0.9741\n",
      "Epoch 11: val_loss did not improve from 0.33639\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "352/352 [==============================] - 27s 77ms/step - loss: 0.0710 - f_score: 0.9732 - precision: 0.9730 - recall: 0.9741 - val_loss: 0.5563 - val_f_score: 0.8397 - val_precision: 0.8714 - val_recall: 0.8123\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "lenet5_v4_best_model = tuner.get_best_models()[0]\n",
    "lenet5_v4_best_model = train_model(lenet5_v4_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d93d0d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 12s 34ms/step - loss: 0.2034 - f_score: 0.9171 - precision: 0.8970 - recall: 0.9409\n",
      "20/20 [==============================] - 1s 34ms/step - loss: 0.3997 - f_score: 0.8284 - precision: 0.8120 - recall: 0.8544\n",
      "train error =  0.08293211460113525\n",
      "test error =  0.17155903577804565\n"
     ]
    }
   ],
   "source": [
    "print_errors(lenet5_v4_best_model, train_seq, test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff392b62",
   "metadata": {},
   "source": [
    "## Lenet5 with three additional conv layers and 256x256 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8718ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq, val_seq, test_seq = get_dataset_sequences('train-256', 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e677f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet5_v5(hp):\n",
    "    \n",
    "    inputs = layers.Input(shape=(256, 256, 3))\n",
    "    nn = layers.Rescaling(1.0 / 255.0)(inputs)\n",
    "\n",
    "    nn = layers.Conv2D(filters=hp.Int(name='filters_conv_1', min_value=1, max_value=20, step=1),\n",
    "                       kernel_size=5, strides=(1, 1), activation='relu')(inputs)\n",
    "    nn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(nn)\n",
    "\n",
    "    \n",
    "    nn = layers.Conv2D(filters=hp.Int(name='filters_conv_2', min_value=20, max_value=40, step=1),\n",
    "                       kernel_size=5, strides=(1, 1), activation='relu')(nn)\n",
    "    nn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(nn)\n",
    "\n",
    "    \n",
    "    nn = layers.Conv2D(filters=hp.Int(name='filters_conv_3', min_value=40, max_value=60, step=1),\n",
    "                       kernel_size=5, strides=(1, 1), activation='relu')(nn)\n",
    "    nn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(nn)\n",
    "    \n",
    "    \n",
    "    nn = layers.Conv2D(filters=hp.Int(name='filters_conv_4', min_value=60, max_value=80, step=1),\n",
    "                       kernel_size=5, strides=(1, 1), activation='relu')(nn)\n",
    "    nn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(nn)\n",
    "    \n",
    "    \n",
    "    nn = layers.Conv2D(filters=hp.Int(name='filters_conv_5', min_value=80, max_value=100, step=1),\n",
    "                        kernel_size=5, strides=(1, 1), activation='relu')(nn)\n",
    "    nn = layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(nn)\n",
    "\n",
    "    \n",
    "    nn = layers.Flatten()(nn)\n",
    "\n",
    "    nn = layers.Dense(hp.Int(name='units_dense_1', min_value=300, max_value=500, step=1), activation='relu')(nn)\n",
    "    nn = layers.Dense(hp.Int(name='units_dense_2', min_value=80, max_value=120, step=1), activation='relu')(nn)\n",
    "    nn = layers.Dense(hp.Int(name='units_dense_3', min_value=40, max_value=80, step=1), activation='relu')(nn)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(nn)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    train_metrics = [f_score,\n",
    "                     metrics.Precision(thresholds=0.5),\n",
    "                     metrics.Recall(thresholds=0.5)]\n",
    "\n",
    "    train_optimizer = optimizers.Adam(\n",
    "        learning_rate=hp.Float(name='learning_rate', min_value=10e-6, max_value=10e-4, sampling='log'))\n",
    "    train_loss = 'binary_crossentropy'\n",
    "\n",
    "    model.compile(loss=train_loss, optimizer=train_optimizer, metrics=train_metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b92dec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 31m 26s]\n",
      "val_loss: 0.22479461133480072\n",
      "\n",
      "Best val_loss So Far: 0.2145802527666092\n",
      "Total elapsed time: 04h 46m 47s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner = tune_model(lenet5_v5, train_seq, val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42167506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.1142 - f_score: 0.9531 - precision: 0.9490 - recall: 0.9600\n",
      "Epoch 1: val_loss improved from inf to 0.22811, saving model to dogs_vs_cats_lenet5_weights_1.h5\n",
      "704/704 [==============================] - 91s 128ms/step - loss: 0.1142 - f_score: 0.9531 - precision: 0.9490 - recall: 0.9600 - val_loss: 0.2281 - val_f_score: 0.9051 - val_precision: 0.9121 - val_recall: 0.9006\n",
      "Epoch 2/100\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.0986 - f_score: 0.9600 - precision: 0.9568 - recall: 0.9657\n",
      "Epoch 2: val_loss did not improve from 0.22811\n",
      "704/704 [==============================] - 90s 128ms/step - loss: 0.0986 - f_score: 0.9600 - precision: 0.9568 - recall: 0.9657 - val_loss: 0.2978 - val_f_score: 0.8880 - val_precision: 0.9324 - val_recall: 0.8486\n",
      "Epoch 3/100\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.0851 - f_score: 0.9668 - precision: 0.9655 - recall: 0.9696\n",
      "Epoch 3: val_loss did not improve from 0.22811\n",
      "704/704 [==============================] - 90s 127ms/step - loss: 0.0851 - f_score: 0.9668 - precision: 0.9655 - recall: 0.9696 - val_loss: 0.2769 - val_f_score: 0.9103 - val_precision: 0.9184 - val_recall: 0.9054\n",
      "Epoch 4/100\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.0761 - f_score: 0.9704 - precision: 0.9700 - recall: 0.9723\n",
      "Epoch 4: val_loss did not improve from 0.22811\n",
      "704/704 [==============================] - 90s 128ms/step - loss: 0.0761 - f_score: 0.9704 - precision: 0.9700 - recall: 0.9723 - val_loss: 0.3159 - val_f_score: 0.9091 - val_precision: 0.9195 - val_recall: 0.9006\n",
      "Epoch 5/100\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.0595 - f_score: 0.9767 - precision: 0.9768 - recall: 0.9777\n",
      "Epoch 5: val_loss did not improve from 0.22811\n",
      "704/704 [==============================] - 90s 128ms/step - loss: 0.0595 - f_score: 0.9767 - precision: 0.9768 - recall: 0.9777 - val_loss: 0.3212 - val_f_score: 0.9050 - val_precision: 0.9091 - val_recall: 0.8991\n",
      "Epoch 6/100\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.0664 - f_score: 0.9752 - precision: 0.9760 - recall: 0.9757\n",
      "Epoch 6: val_loss did not improve from 0.22811\n",
      "704/704 [==============================] - 90s 128ms/step - loss: 0.0664 - f_score: 0.9752 - precision: 0.9760 - recall: 0.9757 - val_loss: 0.3659 - val_f_score: 0.9175 - val_precision: 0.9102 - val_recall: 0.9274\n",
      "Epoch 7/100\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.0483 - f_score: 0.9824 - precision: 0.9824 - recall: 0.9836\n",
      "Epoch 7: val_loss did not improve from 0.22811\n",
      "704/704 [==============================] - 90s 128ms/step - loss: 0.0483 - f_score: 0.9824 - precision: 0.9824 - recall: 0.9836 - val_loss: 0.5202 - val_f_score: 0.8841 - val_precision: 0.9336 - val_recall: 0.8423\n",
      "Epoch 8/100\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.0524 - f_score: 0.9793 - precision: 0.9793 - recall: 0.9816\n",
      "Epoch 8: val_loss did not improve from 0.22811\n",
      "704/704 [==============================] - 90s 128ms/step - loss: 0.0524 - f_score: 0.9793 - precision: 0.9793 - recall: 0.9816 - val_loss: 0.3579 - val_f_score: 0.9260 - val_precision: 0.9231 - val_recall: 0.9274\n",
      "Epoch 9/100\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.0480 - f_score: 0.9818 - precision: 0.9813 - recall: 0.9833\n",
      "Epoch 9: val_loss did not improve from 0.22811\n",
      "704/704 [==============================] - 90s 127ms/step - loss: 0.0480 - f_score: 0.9818 - precision: 0.9813 - recall: 0.9833 - val_loss: 0.3520 - val_f_score: 0.9215 - val_precision: 0.9174 - val_recall: 0.9290\n",
      "Epoch 10/100\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.0426 - f_score: 0.9853 - precision: 0.9838 - recall: 0.9872\n",
      "Epoch 10: val_loss did not improve from 0.22811\n",
      "704/704 [==============================] - 90s 128ms/step - loss: 0.0426 - f_score: 0.9853 - precision: 0.9838 - recall: 0.9872 - val_loss: 0.3981 - val_f_score: 0.9194 - val_precision: 0.9171 - val_recall: 0.9243\n",
      "Epoch 11/100\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.0450 - f_score: 0.9837 - precision: 0.9837 - recall: 0.9846\n",
      "Epoch 11: val_loss did not improve from 0.22811\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "704/704 [==============================] - 90s 127ms/step - loss: 0.0450 - f_score: 0.9837 - precision: 0.9837 - recall: 0.9846 - val_loss: 0.3774 - val_f_score: 0.9109 - val_precision: 0.8794 - val_recall: 0.9432\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "lenet5_v5_best_model = tuner.get_best_models()[0]\n",
    "lenet5_v5_best_model = train_model(lenet5_v5_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14829904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704/704 [==============================] - 36s 50ms/step - loss: 0.1089 - f_score: 0.9618 - precision: 0.9643 - recall: 0.9617\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 0.2571 - f_score: 0.8742 - precision: 0.8976 - recall: 0.9019\n",
      "train error =  0.038176119327545166\n",
      "test error =  0.12582576274871826\n"
     ]
    }
   ],
   "source": [
    "print_errors(tuner.get_best_models()[0], train_seq, test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f454b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dog-vs-cat",
   "language": "python",
   "name": "dog-vs-cat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
